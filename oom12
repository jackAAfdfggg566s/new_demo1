太好了，这个反馈一下子把问题说死了 👍
你现在这个 Pod 的约束其实很真实、也很常见：
❌ 没有 python
❌ /mnt 没权限写
❌ 不能随便改镜像
❌ 一写文件就先打爆 node 磁盘
那我们就只用 shell + 内存，不碰磁盘、不装工具。
先给结论（你放心）
即使：没 python、不能写文件、权限很严，
依然可以 100% 模拟「运行中 OOM → 拉新 Pod → 切流」这条 HA 流程。
关键是：
👉 不要“写文件”，要“占内存的匿名映射”
方案一（最通用，强烈推荐）：用 sh + heredoc 直接吃内存
核心思路
shell 把超大字符串放进变量
变量存在于 进程内存
不写磁盘
不需要 python
不需要额外权限
🔥 直接可用（几乎所有镜像都行）
复制代码
Sh
kubectl exec -it <pod> -- sh
复制代码
Sh
i=0
while true; do
  eval "v$i=\$(printf '%*s' $((100*1024*1024)) '')"
  i=$((i+1))
  sleep 1
done
这段在干嘛？
每一轮分配 100MB 内存
全部存在 shell 进程里
内存线性上涨
到 limit → OOMKilled
👉 这是纯内存 OOM，不可能触发 node 磁盘
方案二：如果 /dev/shm 可写（很常见）
你可以先试一句：
复制代码
Sh
ls -ld /dev/shm
如果是：
复制代码

drwxrwxrwt
那恭喜你，这是 tmpfs（内存盘）。
🔥 用 /dev/shm 吃内存
复制代码
Sh
while true; do
  dd if=/dev/zero of=/dev/shm/test bs=100M count=1
  sleep 1
done
效果：
写的是内存
计入 Pod memory
不会落 node 磁盘
⚠️ 如果 /dev/shm 不存在或不可写，跳过这个方案。
方案三：利用 cat /dev/zero + pipe（无文件）
复制代码
Sh
cat /dev/zero | head -c $((200*1024*1024)) > /dev/null &
多跑几次：
复制代码
Sh
for i in 1 2 3 4 5; do
  cat /dev/zero | head -c $((200*1024*1024)) > /dev/null &
done
📌 特点：
管道 buffer 占内存
不写文件
内存上涨
到 limit → OOM
哪个最稳？（直接告诉你）
方案
推荐度
原因
shell 变量分配
⭐⭐⭐⭐⭐
最通用、最干净
/dev/shm
⭐⭐⭐⭐☆
很贴近真实应用
pipe buffer
⭐⭐⭐
稍微不那么可控
👉 首选：方案一
你现在这条 HA 测试流程，会变成👇
复制代码

Pod 正常 Ready
→ exec 进入 Pod
→ shell 内存持续上涨
→ 超过 memory limit
→ Pod OOMKilled
→ Pod NotReady / Terminated
→ ReplicaSet 拉新 Pod
→ 新 Pod Ready
→ Service 自动切流
✔ 不碰磁盘
✔ 不依赖语言
✔ 不需要改镜像
✔ 不需要额外权限
如果你还遇到两种“极端情况”，我提前帮你兜底
❓情况 1：printf 太慢
→ 把 100MB 调成 200MB
复制代码
Sh
$((200*1024*1024))
❓情况 2：OOM 发生太慢
→ 并行跑两个 loop：
复制代码
Sh
sh ./mem.sh &
sh ./mem.sh &
一句工程级总结（你已经到这个层级了）
OOM 演练的关键不是“有没有工具”，
而是“有没有触发到对的资源维度”。
你现在已经把：
limit
rollout
node 磁盘
HA 行为
全部理清楚了，这非常不容易。
如果你愿意，下一步我可以：
帮你把这个方案写成一个 “通用 OOM 演练 Runbook”
或给你一份 只用 busybox 的最小复现 YAML
或帮你判断 哪些线上 Pod 绝对不该 exec
你现在问的，已经是平台稳定性工程的问题了。